import numpy as np
import sounddevice as sd
import onnxruntime
from time import time as current_time
import hydra 
import sys
import paho.mqtt.client as paho
import json
from utils import FIFOBuffer
import threading 
import datetime

# Use the provided OnnxWrapper and VADIterator classes
class OnnxWrapper():

    def __init__(self, path, force_onnx_cpu=False):
        opts = onnxruntime.SessionOptions()
        opts.inter_op_num_threads = 1
        opts.intra_op_num_threads = 1

        if force_onnx_cpu and 'CPUExecutionProvider' in onnxruntime.get_available_providers():
            self.session = onnxruntime.InferenceSession(path, providers=['CPUExecutionProvider'], sess_options=opts)
        else:
            self.session = onnxruntime.InferenceSession(path, sess_options=opts)

        self.reset_states()
        self.sample_rates = [8000, 16000]

    def _validate_input(self, x, sr: int):
        if x.ndim == 1:
            x = np.expand_dims(x, axis=0)
        if x.ndim > 2:
            raise ValueError(f"Too many dimensions for input audio chunk {x.ndim}")

        if sr != 16000 and (sr % 16000 == 0):
            step = sr // 16000
            x = x[:, ::step]
            sr = 16000

        if sr not in self.sample_rates:
            raise ValueError(f"Supported sampling rates: {self.sample_rates} (or multiply of 16000)")
        if sr / x.shape[1] > 31.25:
            raise ValueError("Input audio chunk is too short")

        return x, sr

    def reset_states(self, batch_size=1):
        self._state = np.zeros((2, batch_size, 128), dtype=np.float32)
        self._context = np.zeros((0, 0), dtype=np.float32)
        self._last_sr = 0
        self._last_batch_size = 0

    def __call__(self, x, sr: int):
        x, sr = self._validate_input(x, sr)
        num_samples = 512 if sr == 16000 else 256

        if x.shape[-1] != num_samples:
            raise ValueError(f"Provided number of samples is {x.shape[-1]} (Supported values: 256 for 8000 sample rate, 512 for 16000)")

        batch_size = x.shape[0]
        context_size = 64 if sr == 16000 else 32

        if not self._last_batch_size:
            self.reset_states(batch_size)
        if self._last_sr and self._last_sr != sr:
            self.reset_states(batch_size)
        if self._last_batch_size and self._last_batch_size != batch_size:
            self.reset_states(batch_size)

        if not self._context.size:
            self._context = np.zeros((batch_size, context_size), dtype=np.float32)

        x = np.concatenate([self._context, x], axis=1)
        if sr in [8000, 16000]:
            ort_inputs = {'input': x.astype(np.float32), 'state': self._state.astype(np.float32), 'sr': np.array(sr, dtype=np.int64)}
            ort_outs = self.session.run(None, ort_inputs)
            out, state = ort_outs
            self._state = state
        else:
            raise ValueError()

        self._context = x[..., -context_size:]
        self._last_sr = sr
        self._last_batch_size = batch_size

        return out

    def audio_forward(self, x, sr: int):
        outs = []
        x, sr = self._validate_input(x, sr)
        self.reset_states()
        num_samples = 512 if sr == 16000 else 256

        if x.shape[1] % num_samples:
            pad_num = num_samples - (x.shape[1] % num_samples)
            x = np.pad(x, ((0, 0), (0, pad_num)), 'constant', constant_values=0.0)

        for i in range(0, x.shape[1], num_samples):
            wavs_batch = x[:, i:i+num_samples]
            out_chunk = self.__call__(wavs_batch, sr)
            outs.append(out_chunk)

        stacked = np.concatenate(outs, axis=1)
        return stacked

class VADIterator:
    def __init__(self,
                 model,
                 threshold: float = 0.5,
                 sampling_rate: int = 16000,
                 min_silence_duration_ms: int = 100,
                 speech_pad_ms: int = 30):

        self.model = model
        self.threshold = threshold
        self.sampling_rate = sampling_rate

        if sampling_rate not in [8000, 16000]:
            raise ValueError('VADIterator does not support sampling rates other than [8000, 16000]')

        self.min_silence_samples = sampling_rate * min_silence_duration_ms / 1000
        self.speech_pad_samples = sampling_rate * speech_pad_ms / 1000
        self.reset_states()

    def reset_states(self):
        self.model.reset_states()
        self.triggered = False
        self.temp_end = 0
        self.current_sample = 0

    def __call__(self, x, buf, return_seconds=False):
        if not isinstance(x, np.ndarray):
            try:
                x = np.array(x, dtype=np.float32)
            except:
                raise TypeError("Audio cannot be casted to numpy array. Cast it manually")
        # print(x.shape)
        if self.triggered:
            buf.enqueue(x.tolist())
        window_size_samples = len(x[0]) if x.ndim == 2 else len(x)
        self.current_sample += window_size_samples

        speech_prob = self.model(x, self.sampling_rate).item()

        if (speech_prob >= self.threshold) and self.temp_end:
            self.temp_end = 0

        if (speech_prob >= self.threshold) and not self.triggered:
            self.triggered = True
            speech_start = self.current_sample - self.speech_pad_samples - window_size_samples
            return {'start': int(speech_start) if not return_seconds else round(speech_start / self.sampling_rate, 1)}

        if (speech_prob < self.threshold - 0.20) and self.triggered:
            if not self.temp_end:
                self.temp_end = self.current_sample
            if self.current_sample - self.temp_end < self.min_silence_samples:
                return None
            else:
                speech_end = self.temp_end + self.speech_pad_samples - window_size_samples
                self.temp_end = 0
                self.triggered = False
                return {'end': int(speech_end) if not return_seconds else round(speech_end / self.sampling_rate, 1)}

        return None





@hydra.main(version_base=None, config_path='./config', config_name='base')
def main(args):
    
    client = paho.Client(paho.CallbackAPIVersion.VERSION1)

    if client.connect(args.MQTT.BROKER_ADDRESS, 1883, 60) != 0:
        print("Couldn't connect to the mqtt broker")
        sys.exit(1)

    def infer_nac(audio_data, session):
        inputs = session.get_inputs()
        input_name = inputs[0].name
        
        print("AUDIODATASHAPE",audio_data.shape)
        audio_data = np.expand_dims(audio_data[:16000],(0,1))

        model_output = session.run(None, {input_name: audio_data})  # Pass audio to model
        return model_output[0]
    
    def audio_callback(indata, frames, time, status):
        if status:
            print(status)
        # start = current_time()
        audio_chunk = np.array(indata[:, 0], dtype=np.float32)  # Convert to numpy array
        result = vad_iterator(audio_chunk, buffer)
        # print('inference time:', current_time()- start, 'use percent:', (current_time()-start)/0.032*100,'%')
        if result is not None:
            print(result)
           
    # AUDIO LOGGING
    def log_audio(buf):
        while True:
            out = buf.dequeue()
            out = infer_nac(np.array(out, dtype=np.float16),nac_model).tolist()
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            out = json.dumps({"id": args.LOGGER_ID, "timestamp":timestamp, "data": out}) 
            client.publish("AUDIOLOGGER", out, 0)
            
        
    # Constants
    SAMPLING_RATE = args.SAMPLING_RATE
    CHUNK_SIZE = args.CHUNK_SIZE  # 512 Corresponds to 32 ms at 16 kHz
    
    # LVAD MODEL
    onnx_model_path = args.SILERO.MODEL_PATH  # Path to the ONNX model file
    onnx_model = OnnxWrapper(onnx_model_path)
    vad_iterator = VADIterator(onnx_model)
    
    # NAC MODEL
    # opts = onnxruntime.SessionOptions()
    # opts.inter_op_num_threads = 1
    # opts.intra_op_num_threads = 1
    nac_path = args.NAC.MODEL_PATH
    nac_model = onnxruntime.InferenceSession(nac_path, providers=['CPUExecutionProvider'])

    buffer = FIFOBuffer(SAMPLING_RATE)
    

    # Thread which handles buffer dequeue and sends data with MQTT Client
    t1 = threading.Thread(target=log_audio, args=(buffer,))
    t1.daemon = True
    t1.start()
    
    # Start the audio stream
    with sd.InputStream(channels=1, samplerate=SAMPLING_RATE, callback=audio_callback, blocksize=CHUNK_SIZE):
        print("Streaming audio from the microphone. Press Ctrl+C to stop.")
        
        sd.sleep(int(30 * 1000))  # Stream for 30 seconds
    
        print("Stream Ended")
        
    exit()
    print("Audio stream stopped.")

if __name__=="__main__":
    main()